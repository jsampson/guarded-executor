<html>
  <head>
    <title>GuardedExecutor Design Overview</title>
    <style type="text/css">
      body { max-width: 60em; margin-left: auto; margin-right: auto }
      .center { text-align: center }
      img { display: block; margin-left: auto; margin-right: auto; height: auto }
      img.object-model { width: 32.07em }
      img.state-diagram { width: 27.25em }
      img.formula-1 { width: 4.65em }
      img.formula-2 { width: 47.2em }
      img.formula-3 { width: 43.2em }
      img.formula-4 { width: 6.3em }
      img.formula-5 { width: 13em }
    </style>
  </head>
  <body>
    <h1 class="center">GuardedExecutor Design Overview</h1>
    <p class="center">Justin T. Sampson &lt;<a href="mailto:justin@krasama.com">justin@krasama.com</a>&gt;</p>
    <h2>Introduction</h2>
    <p><code>GuardedExecutor</code> is a high-performance implicit-signalling monitor implementation for Java 8+. This document provides a brief overview of its design.</p>
    <p>The essence of the <code>GuardedExecutor</code> interface starts with the standard <code>Executor</code> interface:</p>
    <p><code>void execute(Runnable task)</code></p>
    <p>It then adds a &ldquo;guarded&rdquo; form of the same method:</p>
    <p><code>void executeWhen(BooleanSupplier guard, Runnable task)</code></p>
    <p>Both methods acquire an exclusive lock, encapsulated within the executor, and then execute the given task before releasing the lock and returning. The guarded form additionally waits until the guard becomes satisfied before executing the task.</p>
    <p>Additional methods appear in the interface to provide a variety of options regarding interruptibility, timeouts, and returning a value by giving a <code>Supplier</code> rather than a <code>Runnable</code>. Even <code>execute(task)</code> is semantically equivalent to <code>executeWhen(() -&gt; true, task)</code>, so the design can be understood by focusing only on the <code>executeWhen</code> method.</p>
    <h2>Performance Benefits</h2>
    <p>Besides its interface being more programmer-friendly, <code>GuardedExecutor</code> turns out to have several performance benefits as well.</p>
    <p>The key to <code>GuardedExecutor</code>&rsquo;s robustness under high contention is that any thread might execute the tasks of other threads, which reduces the amount of task switching required. The executor contains a single, unified queue, in which every node holds both the guard and the task submitted by a waiting thread. This queue is managed in a strictly sequential order: The task that is executed at any moment is the earliest task in the queue that has a currently-satisfied guard.</p>
    <p>Additional benefits of a unified queue include:</p>
    <ul>
      <li>With <code>synchronized</code> or <code>ReentrantLock</code>, a thread must first block to acquire the lock, then awaken when the lock is available, and then evaluate its guard, and then block again if the guard evaluated as false. With <code>GuardedExecutor</code>, a thread will often block just a single time and find its task already executed when it awakens.</li>
      <li>With <code>synchronized</code> or <code>ReentrantLock</code>, a thread that encounters a timeout or interrupt must reacquire the lock before continuing, which is itself an untimed and uninterruptible action. With <code>GuardedExecutor</code>, such a thread continues immediately without blocking again.</li>
    </ul>
    <p><code>GuardedExecutor</code> actually prefers to let the <i>most recent</i> thread in the queue acquire the lock, including letting a newly-arrived thread &ldquo;barge&rdquo; the lock without first being added to the queue, because it will then be responsible for executing all eligible tasks before its own task in the queue. This maximizes the number of tasks executed per context switch, and therefore minimizes the number of context switches required for a given level of throughput, while at the same time maintaining perfect fairness. A thread never executes tasks <em>after</em> its own task in the queue, which is another factor in maintaining fairness&mdash;one thread cannot become stuck doing all of the executor&rsquo;s work.</p>
    <h2>Implementation</h2>
    <p>Because of this bias toward the last thread in the queue, the implementation of the queue itself can be extremely simple. New nodes are added to the queue by atomically updating the tail pointer, and whenever one thread releases the lock it simply unparks the thread currently at the tail.</p>
    <img class="object-model" src="object-model.png">
    <p>In this diagram, bold names are volatile fields and italic names are non-volatile fields. Notice that most of the <code>Node</code> class&rsquo;s fields are non-volatile. The implementation is coded carefully to minimize volatile accesses. There are some racy accesses, which are nonetheless correct due to causality. Every volatile field is only accessed in a volatile (never racy) manner, and every non-volatile field is only accessed in a non-volatile (sometimes racy) manner.</p>
    <p>The <code><strong>state</strong></code> field is initially 0. To acquire the lock, a thread must successfully compare-and-set its value from 0 to 1. To release the lock, its value is reset to 0 with a volatile write.</p>
    <p>The <code><strong>tail</strong></code> field is initially null. Thereafter it is only ever set to a non-null value using compare-and-set. While nodes may be removed from within the queue once they are cancelled or executed, a node is never removed while it is at the tail itself. Therefore the <code><strong>tail</strong></code> field moves strictly forward.</p>
    <p>A node&rsquo;s <code><strong>status</strong></code> field is initially 0 (a.k.a. <code>WAITING</code>) and follows these possible transitions:</p>
    <img class="state-diagram" src="state-diagram.png">
    <p>All of the <code>Node</code> class&rsquo;s other fields are non-volatile, but they are initialized before the node is added to the queue, so their initial visibility piggy-backs on the volatile write to <code><strong>tail</strong></code>.</p>
    <p>Once initialized, <code><em>prev</em></code> is only ever modified by a thread holding the lock, and only to point to another node earlier in the queue (or null). Therefore, whenever it is read by a thread that is <em>not</em> holding the lock, the read is racy but still safe as long as that thread is traversing the queue starting from a volatile read of <code><strong>tail</strong></code>. Such a volatile read of <code><strong>tail</strong></code> ensures that the initial values of the tail node <em>and</em> all earlier nodes are safely visible. Since any given read of any such node&rsquo;s <code><em>prev</em></code> field, even if racy, can only possibly point to some node earlier in the queue, the traversal always goes in the right direction and always visits nodes that are safely visible to the thread doing the traversal.</p>
    <p>The <code><em>next</em></code> field is purely for temporary bookkeeping done by a thread that is executing tasks for other threads on the queue. Since that requires holding the lock, it is never subject to races.</p>
    <p>The <code><em>thread</em></code> and <code><em>guard</em></code> fields are only modified by nulling them out when cancelling or executing the node. Therefore a racy read will only ever see the initial value or null. Several monitoring methods read <code><em>thread</em></code> in a racy manner, which is safe as long as they check for null; seeing null implies, causally, that the node has been, or is being, cancelled or executed already, even though there is no guaranteed ordering between a read of <code><em>thread</em></code> and a read of <code><strong>status</strong></code>. The <code><em>guard</em></code> field can be null initially, which means that seeing null does <em>not</em> imply that the node has been cancelled or executed already; but the only place that it is read is robust to this fact.</p>
    <p>Finally, the <code><em>task</em></code> field is never actually accessed in a racy manner. It serves double duty, holding the task object before it is executed and then holding the result returned by the task after it is executed until it is consumed. It is only read by a thread that is about to execute another thread&rsquo;s task or by a thread whose task has been executed by another thread. It is only written in those same scenarios or by a thread that is cancelling its node, which can only happen if it is definitely not being executed. The visibility of the result of such an execution piggy-backs on a volatile write of <code><strong>status</strong></code> (to either <code>RETURNED</code> or <code>THREW</code>).</p>
    <h2>Performance Testing Methodology</h2>
    <p>I&rsquo;m interested in comparing both the <em>throughput</em> and the <em>fairness</em> of <code>GuardedExecutor</code> as compared to <code>ReentrantLock</code> in steady-state execution, not simply the time to complete some number of operations. To that end, I have developed a performance testing framework that, for each of a variety of scenarios, starts up some number of threads and <em>samples</em> each thread&rsquo;s throughput over a fixed duration. For example, a single trial may run for 2s divided into 10 samples of 200ms each.</p>
    <p>The scenarios being tested are combinations of the following variables, with multiple trials per scenario, and with trials across all scenarios shuffled together randomly:</p>
    <ul>
      <li><code>synchronized</code> <em>vs.</em> <code>ReentrantLock</code> (fair and non-fair) <em>vs.</em> Guava&rsquo;s <code>Monitor</code> (fair and non-fair) <em>vs.</em> <code>GuardedExecutor</code></li>
      <li>different numbers of threads (<em>e.g.</em> 1, 2, 20, 100)</li>
      <li>different simulated workloads (computation done in a thread between operations, <em>e.g.</em> 20ns <em>vs.</em> 1000ns, with random jitter included)</li>
      <li>simple locking (no conditions) <em>vs.</em> producer/consumer (two different conditions)</li>
      <li>for producer/consumer scenarios, different blocking queue capacities (<em>e.g.</em> 1, 3, 10)</li>
    </ul>
    <p>(A &ldquo;baseline&rdquo; test subject, which just updates an <code>AtomicLong</code> rather than implementing an actual lock, is run through the same performance testing framework, and is included in the report alongside <code>GuardedExecutor</code> <em>et al.</em>, as a sanity check.)</p>
    <p>Each test thread counts the number of operations that it has performed, and periodically (say, every 100 operations) writes that count to a volatile field dedicated to that thread. The main thread reads the volatile counter for each test thread at the start and end of every sample period, and records the throughput for that thread in that sample as the delta of its counter divided by the duration of the sample. The test threads are allowed to run for a warmup period before the first sample is measured, and run continuously until after the last sample is measured in order to simulate steady-state execution.</p>
    <p>A trial with <em>T</em> test threads and <em>S</em> sample periods therefore produces <em>N</em>&nbsp;=&nbsp;<em>TS</em> total measurements. The mean <em>&mu;</em> of those measurements represents the average throughput per thread, and therefore the throughput for the trial as a whole is reported as <em>T&mu;</em>.</p>
    <p>To assess the fairness of each implementation, it is not enough to simply describe the logical fairness of their designs. A default <code>ReentrantLock</code> usually lets threads acquire the lock in the order that they are added to the queue, but occasionally allows threads to &ldquo;barge&rdquo; the lock for the sake of increased throughput, whereas a &ldquo;fair&rdquo; <code>ReentrantLock</code> ensures that threads acquire the lock in the order that they are added to the queue&mdash;but even a &ldquo;fair&rdquo; <code>ReentrantLock</code> loses track of a thread&rsquo;s place in the queue when it awaits a condition. <code>GuardedExecutor</code> goes further, maintaining the order of tasks in the queue regardless of whether each task has a guard or not, and thus is <em>a priori</em> the fairest of the three. However, in a running system under heavy load, individual threads may become so starved for processor time that they don&rsquo;t even have a chance to get added to the queue, regardless of lock implementation. Therefore, it is necessary to measure <em>effective</em> fairness empirically.</p>
    <p>One obvious way to report fairness is the standard deviation <em>&sigma;</em> of the throughput measurements. Since the standard deviation will naturally be larger for trials with higher throughput, one improvement is to instead use the coefficient of variation <em>&sigma;</em>&nbsp;&frasl;&nbsp;<em>&mu;</em>. Still, I found it hard to get an intuitive grasp of the relative significance of different coefficients of variation, since, like standard deviations, they range from a best case of 0 to a worst case of &infin;. What I really want to see is the proportion of threads that are making progress, which should range from a worst case of 0% to a best case of 100%.</p>
    <p>To that end, imagine a simplified scenario in which <em>P</em> of the <em>N</em> total measurements show threads making equal progress with throughput <em>&tau;</em> and the remaining <em>N</em>&nbsp;&minus;&nbsp;<em>P</em> measurements show threads making no progress with throughput 0. Then we have</p>
    <img class="formula-1" src="formula-1.png">
    <p>and</p>
    <img class="formula-2" src="formula-2.png">
    <p>Therefore,</p>
    <img class="formula-3" src="formula-3.png">
    <p>and finally</p>
    <img class="formula-4" src="formula-4.png">
    <p>and</p>
    <img class="formula-5" src="formula-5.png">
    <p>Since <em>P</em>&nbsp;&frasl;&nbsp;</span><em>N</em> is the proportion being sought, this equation suggests the definition of effective fairness as 1&nbsp;&frasl;&nbsp;((<em>&sigma;</em>&nbsp;&frasl;&nbsp;<em>&mu;</em>)<sup>2</sup>&nbsp;+&nbsp;1), which is easy to calculate from the coefficient of variation <em>&sigma;</em>&nbsp;&frasl;&nbsp;<em>&mu;</em> even when the measurements are not so uniform.</p>
    <h2>Performance Test Results</h2>
    <p><code>GuardedExecutor</code> has slightly larger constant overhead per operation than <code>ReentrantLock</code>, which shows up in all of the 1-thread scenarios with very small workloads. That is probably due partly to <code>GuardedExecutor</code>&rsquo;s indirection through lambdas and partly to <code>ReentrantLock</code>&rsquo;s use of non-standard internal APIs of the JVM, though when I have attempted to control for those differences the results have been inconclusive. In both classes, uncontended locking basically comes down to a single compare-and-set to acquire the lock and a single volatile write to release it, so any overhead differences may very well just be differences of coding style and compiler optimization. In any case, the differences are small enough that they disappear with larger workloads and under contention with multiple threads.</p>
    <p>Remember that <code>GuardedExecutor</code> is <em>a priori</em> fairer than either a fair or a non-fair <code>ReentrantLock</code>. The empirical results are consistent with that expectation, with <code>GuardedExecutor</code> showing near-100% effective fairness in every single scenario. What may be surprising is that a &ldquo;fair&rdquo; <code>ReentrantLock</code> does <em>not</em> show consistently higher effective fairness than a &ldquo;non-fair&rdquo; one! Both of them tend to suffer in producer/consumer scenarios under contention, with their effective fairness sometimes dropping below 50%. In several scenarios, the &ldquo;non-fair&rdquo; version actually does quite a bit better than the &ldquo;fair&rdquo; version. Perhaps the non-fair implementation supports better CPU utilization overall, which lets <em>all</em> threads make more progress.</p>
    <p>Even though a &ldquo;fair&rdquo; <code>ReentrantLock</code> is not consistently fairer than a &ldquo;non-fair&rdquo; one, it <em>is</em> consistently <em>slower</em>, except in the lowest-contention scenarios. It turns out that a &ldquo;fair&rdquo; <code>ReentrantLock</code> is <em>also</em> consistently slower than a <code>GuardedExecutor</code>, again except in the lowest-contention scenarios. Therefore it appears that <code>GuardedExecutor</code> is the superior choice for any use cases that care primarily about fairness.</p>
    <p>Finally, a &ldquo;non-fair&rdquo; <code>ReentrantLock</code> offers higher throughput than a <code>GuardedExecutor</code> <em>only</em> without contention or in simple locking scenarios. In producer/consumer scenarios with even light contention, <code>GuardedExecutor</code> is the clear winner. <code>GuardedExecutor</code>&rsquo;s ability to handle increasing load gracefully by having threads execute tasks for each other means that its performance is more robust overall.</p>
    <h2>Alternative Design Choices</h2>
    <p>Along the way to the current <code>GuardedExecutor</code> design described above, I&rsquo;ve also experimented with some alternative design choices. The design that I&rsquo;ve arrived at is not necessarily the strongest in every single scenario, but is the most robust overall and therefore seems to be the best design to move forward into peer review and official release.</p>
    <p><em>Executing tasks in the caller thread:</em> This was my starting point, essentially taking the ideas from Guava&rsquo;s <code>Monitor</code> and reimplementing from the ground up instead of wrapping a <code>ReentrantLock</code> as <code>Monitor</code> does. This design had a bit higher throughput in low- contention scenarios, but was less robust under high contention. Its queue structure and signalling logic were actually more complicated as well, since it attempted to find the next thread having a satisfied guard rather than simply unparking the latest thread as the current implementation does. This implementation also allowed barging to improve throughput, so tasks were not always processed strictly in queue order.</p>
    <p><em>Executing tasks in a dedicated thread:</em> This implementation was intended to be the &ldquo;fair&rdquo; version of the first implementation, by executing all tasks strictly in queue order. That was accomplished by having a single dedicated thread that did nothing but execute such tasks. The effective fairness of this design was nearly perfect. Interestingly, its throughput was extremely consistent: No matter how many threads were submitting tasks, it maintained a near-constant throughput. For large numbers of threads, that actually made this design the throughput leader. Unfortunately, for small numbers of threads that same property made it much <em>slower</em> than the other options. Also, properly maintaining a dedicated thread would have required expanding the API to support thread factories and shutdown semantics.</p>
    <p>The experience of implementing those first two design choices strongly suggested the current <code>GuardedExecutor</code> design. Under low contention, tasks are almost always executed in the caller thread, while under high contention, tasks are almost always executed in batches. This achieves the best properties of both approaches: near-perfect fairness <em>and</em> robust throughput under contention.</p>
  </body>
</html>
